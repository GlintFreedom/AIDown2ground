# 梯度下降

### 用途

计算损失函数最小值（找全局最优）

![](.\梯度下降.webp)

### 概念

1. 方向导数

   对一元函数$y=f(x)$来说，导数就是函数的变化率，从几何意义上看就是：

   ![](.\一元函数的导数.webp)

   而对多元函数$y=f(x_0,x_1,...)$来说，过任何一点$(x_0,x_1,...,y)$都有无数条曲线存在于超平面内，而每条曲线有对应的导数，因此从这个意义上看，**多元函数过任何一点，需要在“某个方向”的限制下，有对应的导数，即方向导数。**

   ![](.\全导数.webp)

   导数的本质是变化率，而方向导数就是一个指定了某一个方向的变化率。其中，偏导数就是在某点关于x轴方向或y轴方向上的导数。

   具体的，**方向导数的方向是关于自变量的一条射线**。

   例如，在二元函数$z=f(x,y)$中，过$(x_0,y_0)$点有无数个方向$(x-x_0,y-y_0)$，选择其中一个单位向量为$\overrightarrow{e_l}=(\cos \alpha,\cos \beta)$的方向$l$，则该方向向量可以表示为：
   $$
   \left. \frac{ {\partial}f}{ {\partial}l} \right| _{(x_0,y_0)}=f'_x(x_0,y_0)\cos \alpha+f'_y(x_0,y_0)\cos \beta
   $$
   
2. 梯度

   **梯度是过某点最大的方向导数**，可以被证明为：
   $$
   \nabla f(\mathbf{x})=(\left.\frac{ {\partial}y}{ {\partial}x_0}\right| _{\mathbf{x}},\left.\frac{ {\partial}y}{ {\partial}x_1}\right| _{\mathbf{x}},...)=(f_{x_0}(\mathbf{x}),f_{x_1}(\mathbf{x}),...)
   $$

### 数理推导

设损失函数$Loss=f(\mathbf{x})$，则要找到全局最优点$\mathbf{x_\alpha}$，使得$f(\mathbf{x_\alpha})$最小

对任意点$\mathbf{x}=\{x_0,x_1,x_2,...\}$，有$f(\mathbf{x})$

则其梯度$\nabla f(\mathbf{x})=(f_{x_0}(\mathbf{x}),f_{x_1}(\mathbf{x}),...)$

求梯度的一般方法如下：

```python
def function(*args):
    # 一个简单的多变量函数的示例，args为目标点(x0,x1,x2,...)
    return sum(x**2 for x in args)


def compute_gradient(func, *args):
    epsilon = 1e-6
    gradients = []

    for i, arg in enumerate(args):
        # 对每个自变量分别计算偏导数
       	# perturbed_args=[x0,x1,x2,x3+epsilon,x4,x5,...]
        perturbed_args = [a + epsilon if j == i else a for j, a in enumerate(args)]
        gradient = (func(*perturbed_args) - func(*args)) / epsilon
        gradients.append(gradient)

    return gradients
```

### 梯度下降法

在具体任务中，存在三种不同的梯度下降方法

- 批量梯度下降：在每次迭代中，使用整个训练集来计算梯度并更新参数。这种方式的优势在于参数的更新更加准确，但计算梯度时需要遍历整个训练集，计算成本较高，特别是在大规模数据集上。
- 随机梯度下降：在每次迭代中，仅使用一个样本来计算梯度和更新参数。这样计算成本低，但参数更新较为不稳定，可能引入噪声。
- **小批量梯度下降**：实际训练广泛使用小批量梯度下降方法。在每次迭代中，使用一个小批量（mini-batch）的样本来计算梯度和更新参数。小批量梯度下降综合了批量梯度下降的稳定性和随机梯度下降的计算效率。

以小批量梯度下降为基础的优化器Optimizer主要用于反向传播（计算梯度）和参数更新（梯度下降）。

##### PyTorch中的optim.SGD

```python
class SGD:
    # 接受模型的参数 params、学习率 lr、动量参数 momentum 和权重衰减参数 weight_decay
    # params = [fc1.weights, fc1.bias, fc2.weights, fc2.bias, ...]
    # 学习率lr 决定了在每个batch中模型参数更新的步长。常见的lr取值为1e-5~1e-3，也常用动态学习率
    # 动量参数momentum 引入了历史梯度信息，起到平滑和加速梯度更新的作用。常见的momentum取值为0.9或0.99
    # 权重衰减参数weight_decay 是一种正则化技术，通过在损失函数中添加一个关于权重的惩罚项来降低模型的复杂性，防止过拟合。常见的weight_decay取值为1e-5~1e-3，值越小权重衰减效果越强
    def __init__(self, params, lr=0.01, momentum=0, weight_decay=0):
        self.params = list(params)
        self.lr = lr
        self.momentum = momentum
        self.weight_decay = weight_decay
        # 初始动量为0
        self.velocity = [torch.zeros_like(param) for param in self.params]

    def step(self):
        for i, param in enumerate(self.params):
            # 计算梯度
            grad = param.grad
            if self.weight_decay != 0:
                grad = grad.add(self.weight_decay, param)

            # 更新动量 = 累积动量 + 梯度最大的反方向 * 学习率
            self.velocity[i] = self.momentum * self.velocity[i] - self.lr * grad
            # self.velocity[i] = self.momentum * self.velocity[i] - self.lr * grad * (1 - self.momentum) ?

            # 更新参数
            param.data.add_(self.velocity[i])
```

##### PyTorch中的optim.Adam

Adam（Adaptive Moment Estimation）是一种常用的优化算法，能够自适应地调整每个参数的学习率，同时结合了动量的思想，使得更新更加平滑：

1. 初始化模型参数 $\theta$ 和两个累积变量 $m_0 = 0$ 和 $v_0 = 0$，分别用于存储梯度的一阶矩和二阶矩。

2. 在每个迭代步骤$t$中，计算梯度$\nabla \text{Loss} \theta_t$

3. 更新一阶矩$m_t$和二阶矩 $v_t$：

   $$
   m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot \nabla \text{Loss}(\theta_t) \\
   v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot (\nabla \text{Loss}(\theta_t))^2
   $$
   其中，$\beta_1$和$\beta_2$ 是动量和梯度二阶矩的衰减系数，通常分别设置为接近 1 的值，如 0.9 和 0.999。

4. 为了消除在迭代初期对一阶矩和二阶矩的偏差，对一阶矩和二阶矩进行偏差修正：

   $$
   \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \\
   \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
   $$

5. 更新模型参数：

   $$
   \theta_{t+1} = \theta_t - \frac{lr \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
   $$
   其中，$\epsilon$ 是为了数值稳定性而添加的小常数，通常取1e-8。

```python
class Adam(torch.optim.Optimizer):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):
        defaults = {"lr": lr, "betas": betas, "eps": eps}
        super(Adam, self).__init__(params, defaults)

    def step(self, closure=None):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad.data
                state = self.state[p]

                # 初始化状态
                if len(state) == 0:
                    state["step"] = 0
                    state["m"] = torch.zeros_like(p.data)
                    state["v"] = torch.zeros_like(p.data)

                beta1, beta2 = group["betas"]
                state["step"] += 1

                # 计算梯度一阶矩和二阶矩
                state["m"] = beta1 * state["m"] + (1 - beta1) * grad
                state["v"] = beta2 * state["v"] + (1 - beta2) * grad ** 2

                # 偏差修正
                m_hat = state["m"] / (1 - beta1 ** state["step"])
                v_hat = state["v"] / (1 - beta2 ** state["step"])

                # 参数更新
                p.data = p.data - group["lr"] * m_hat / (torch.sqrt(v_hat) + group["eps"])

        return loss
```

在实际使用中，可以根据具体问题对学习率、衰减系数等超参数进行调整。
