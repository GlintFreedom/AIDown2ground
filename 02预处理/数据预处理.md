# 数据预处理

目标：使原始数据变得更适合输入到神经网络中进行训练

### 归一化

将数据缩放到一个标准的范围，通常是[0, 1]或[-1, 1]。这有助于确保不同特征的数值范围不会对训练产生不均衡的影响，避免某些特征权重过大而影响模型的学习。

常见的归一化为**最大最小标准化**：$\mathbf{x}_{normalized}=\frac{\mathbf{x}-min(\mathbf{x})}{max(\mathbf{x})-min(\mathbf{x})}$

```python
import numpy as np

# 原始数据
raw_data = np.array([[2, 5, 10],
                     [15, 25, 30],
                     [8, 12, 18]])

# 计算每列的最小值和最大值
min_vals = np.min(raw_data, axis=0)
max_vals = np.max(raw_data, axis=0)

# 归一化
normalized_data = (raw_data - min_vals) / (max_vals - min_vals)

# 输出结果
print("原始数据：\n", raw_data)
print("\n归一化后的数据：\n", normalized_data)
```

输出：

```shell
原始数据：
 [[ 2  5 10]
 [15 25 30]
 [ 8 12 18]]

归一化后的数据：
 [[0.         0.         0.        ]
 [1.         1.         1.        ]
 [0.46153846 0.35       0.4       ]]
```

### 标准化

将数据调整为均值为0，标准差为1的分布。标准化有助于处理特征之间的不同尺度，使模型更容易学习权重。

标准化实质上包含归一化的内容，一般采用**Z-Score标准化**：$\mathbf{x}_{standardized}=\frac{\mathbf{x}-\mu}{\sigma}$，其中$\mu$为数据的均值，$\sigma$为标准差

```python
import numpy as np

# 原始数据
raw_data = np.array([[2, 5, 10],
                    [15, 25, 30],
                    [8, 12, 18]])

# 计算均值和标准差
mean_vals = np.mean(raw_data, axis=0)
std_dev_vals = np.std(raw_data, axis=0)

# Z-Score Normalization
normalized_data = (raw_data - mean_vals) / std_dev_vals

# 输出结果
print("原始数据：\n", raw_data)
print("\n标准化后的数据：\n", normalized_data)
```

输出：

```shell
原始数据：
 [[ 2  5 10]
 [15 25 30]
 [ 8 12 18]]
 
标准化后的数据：
 [[-1.19216603 -1.08609928 -1.13554995]
 [ 1.25491161  1.32745468  1.29777137]
 [-0.06274558 -0.2413554  -0.16222142]]
```

不进行归一化或标准化会出现一系列问题：

- 收敛速度慢
- 泛化能力差
- **出现梯度爆炸或梯度消失**

**Pytorch中的Batch Normalization层**

```python
import torch
import torch.nn as nn

# 原始数据
raw_data = torch.tensor([[2, 5, 10],
                           [15, 25, 30],
                           [8, 12, 18]], dtype=torch.float32)

# 创建 BatchNorm1d 层
batch_norm = nn.BatchNorm1d(num_features=input_data.size(1))  # num_features 设置为特征的数量

# 将数据传递给 BatchNorm1d 层
normalized_data = batch_norm(input_data)
```

**BatchNorm1d**的具体实现

在确定的、精确的、不需要训练的归一化标准化的基础上，**Batch Normalization层中引入学习缩放参数 `gamma` 和平移参数 `beta`，模型可以根据具体的任务和数据动态地调整每个特征的重要性和偏移。这使得Batch Normalization更适应不同的数据分布，提高了模型的表达能力**。

```python
class BatchNorm1d(nn.Module):
    def __init__(self, num_features, momentum=0.1, eps=1e-5):
        super(BatchNorm1d, self).__init__()
        self.num_features = num_features
        # momentum用于控制对均值和方差的指数加权移动平均。通过保持历史统计信息的加权平均，可以使模型在训练过程中更加稳定。
        self.momentum = momentum
        # 通常取值在0到1之间。较小的momentum会使移动平均更加敏感，而较大的momentum则会减缓更新速度。
        self.eps = eps

        # 可学习参数
        # 缩放参数gamma用于按元素对标准化后的输入进行缩放，从而增加或减小特征的影响。这使得模型可以在训练过程中自适应地学习每个特征的重要性。
        self.gamma = nn.Parameter(torch.ones(num_features))
        # 平移参数beta用于按元素对标准化后的输入进行平移，模型可以更灵活地适应输入数据的整体平移。
        self.beta = nn.Parameter(torch.zeros(num_features))

        # 非可学习参数
        # 注册一个不需要梯度更新的缓冲区，分别保存均值的移动平均值和方差的移动平均值，即运行均值和运行方差
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

    def forward(self, x):
        if self.training:
            # 计算批次上的均值和方差
            batch_mean = x.mean(dim=0)
            batch_var = x.var(dim=0, unbiased=False)

            # 更新运行均值和方差
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var

            # 标准化
            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)

            # 缩放和平移
            output = self.gamma * x_normalized + self.beta
        else:
            # 在测试阶段使用运行的均值和方差进行标准化
            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)
            output = self.gamma * x_normalized + self.beta

        return output
```

### 数据增强

数据增强的目标是引入对抗样本和模型泛化的变化，从而提高模型的鲁棒性和性能。在选择数据增强方法时，需要考虑任务的特点以及数据的分布。对于很多任务，数据增强是提高模型性能的重要手段。

- 图像数据：翻转、旋转、颜色变换、放缩、裁剪等
- 文本数据：随机替换删除word等
- 时间序列数据：时间序列插值等
- 图数据：随机删边、随机删点、随机扰动等

```python
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# 定义数据增强操作
data_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(), # 水平翻转
    transforms.RandomVerticalFlip(), # 垂直翻转
    transforms.RandomRotation(degrees=(-45, 45)), # 随机旋转
    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.9, 1.1)), # 随机缩放
    transforms.RandomCrop(size=(224, 224)), # 随即裁剪
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), # 亮度、对比度和饱和度调整
    transforms.RandomChoice([transforms.Grayscale(), transforms.ColorJitter()]), # 颜色空间变换
    transforms.RandomNoise(mean=0, std=0.1), # 添加噪声
    transforms.ElasticTransform(alpha=1.2, sigma=0.1), # 变形
    transforms.RandomCutout(num_holes=1, max_h_size=50, max_w_size=50), # 随机遮挡矩形区域(Cutout)
    transforms.RandomMixup(alpha=0.2) # 对两张图像进行线性插值，生成新的图像和标签(Mixup)
])

# 加载数据集
dataset = datasets.ImageFolder(root='path_to_dataset', transform=data_transform)

# 创建 DataLoader
batch_size = 32
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# 使用 DataLoader 进行迭代
for batch_data, batch_labels in dataloader:
    # batch_data 是当前批次的输入数据
    # batch_labels 是当前批次的标签
    pass
```

其中，如果未指定`transforms`中增强操作的参数`num_samples`，默认增强前后的数据数量一致。

`dataset`要求`datasets.ImageFolder`的数据格式如下：

```shell
path_to_dataset/class1/image1.jpg
path_to_dataset/class1/image2.jpg
...
path_to_dataset/class2/image1.jpg
path_to_dataset/class2/image2.jpg
...
```

`dataloader`本质上是一个数据迭代器，可以将大规模数据集分成小批次，逐批次地处理数据，同时支持多进程并行加载数据，提高数据加载的效率。

