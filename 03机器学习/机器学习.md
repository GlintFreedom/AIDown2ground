# 机器学习

一个通用的机器学习任务由**模型 model、损失函数 loss、优化器 optimizer**组成，动态的学习过程包括**前向传播、计算损失、后向传播、参数更新**。

### epoch和batch

在具体的深度学习任务中，对全部数据集训练一轮是不够的，反复多轮训练称为多个epoch。多个 epoch是必要的，模型可以多次学习整个数据集，从而更好地学习数据的特征。

在每个 epoch 中，数据集被划分成若干个批次（batches），这样做有显著的好处：

- **加速计算：** 批处理允许在每次迭代中并行处理多个样本，从而加速计算。每个batch中的样本共享相同的参数，每个batch参数更新一次。
- **稳定优化：** 小批次的随机性有助于模型更快地收敛，并且在训练过程中引入了一些噪声，有助于避免陷入局部最小值。
- **内存效率：** 大型数据集无法一次性加载到内存中，批处理大小可以调整以适应内存限制。

### 学习过程

在每个 epoch 中，数据集被划分成若干个批次（batches）。对于每个批次，都会执行以下步骤：

1. **前向传播：** 使用当前模型参数对批次的输入数据进行前向传播，得到模型的预测结果。

2. **计算损失：** 将模型的预测结果与批次的真实标签比较，计算损失。损失是一个标量，表示模型的性能。

3. **反向传播：** 使用损失对模型参数进行反向传播，计算每个参数对于损失的梯度。**在 PyTorch 中，梯度的计算是由自动微分（autograd）自动完成的。在前向传播时，PyTorch会记录每个参数参与的计算，然后在反向传播时使用链式法则计算梯度。**

   > 链式传播，前向和后向时参数到底怎么记录怎么梯度下降的，后续还可以继续深入挖掘一下...

4. **参数更新：** 使用优化器（如SGD、Adam）根据梯度更新模型的参数。优化器会根据学习率和梯度信息来决定参数的更新步长和方向。

这个过程在每个批次中重复进行，直到遍历完整个数据集。一次完整遍历数据集的过程称为一个 epoch。

在整个训练过程中，前向传播、计算损失、反向传播这三个步骤是为了计算梯度信息，而参数更新是真正更新模型参数的步骤。每次参数更新都基于当前批次的梯度信息，而不是整个数据集的平均梯度。这种批次训练的方式有助于提高训练效率和降低计算成本。

### 模型 model

模型由网络架构和参数组成。

网络架构需要初始化各种网络层，并在前向传播`forward`中设计网络层结构。

参数保存在`model.state_dict`中，可以通过`key`调用查看。各种网络层默认的初始化方法是`nn.init.uniform_`，可将权重初始化为正态分布的随机值。

```python
class SimpleNN(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, output_size)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = self.sigmoid(x)
        return x

# 创建模型实例
model = SimpleNN(input_size=8, hidden_size1=64, hidden_size2=32, output_size=1)

# 获取模型的状态字典
model_state_dict = model.state_dict()

# 打印模型的状态字典的键
print(model_state_dict.keys())
```

输出

```shell
odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
```

### 损失函数 loss

主要用于计算预测值和实际label之间的差距（损失）。

##### 均方误差损失MSE

均方误差损失主要用于回归问题（房价预测、温度预测等），用于衡量模型的预测与实际数值之间的差异。
$$
MSE = \frac{1}{n} * \sum_{i=1}^n(y_i - \hat{y_i})^2
$$

```python
# 使用nn.MSELoss计算均方误差
criterion = nn.MSELoss()
		
# 计算损失
loss = criterion(outputs, labels)
```

##### PyTorch中的nn.MSELoss

```python
import torch
import torch.nn as nn

class MSELoss(nn.Module):
    #  reduction 参数用于指定如何对损失值进行汇总，默认值mean计算损失平均值，而sum用于计算损失总和
    def __init__(self, reduction='mean'):
        super(MSELoss, self).__init__()
        self.reduction = reduction

    def forward(self, input, target):
        # 计算平方差
        squared_diff = (input - target)**2

        # 根据 reduction 参数计算损失
        if self.reduction == 'mean':
            loss = torch.mean(squared_diff)
        elif self.reduction == 'sum':
            loss = torch.sum(squared_diff)
        elif self.reduction == 'none':
            loss = squared_diff
        else:
            raise ValueError("Invalid value for reduction. Use 'mean', 'sum', or 'none'.")

        return loss
```

##### 交叉熵损失 Cross-Entropy Loss

用于分类问题，尤其是多分类问题，衡量模型对于每个类别的预测与实际标签之间的差异。
$$

$$
