# 神经网络模型

### 全连接层(FC Layer)

全连接层用于整合高阶特征并进行最终的分类或回归决策。本质上是全部节点共享信息。

假设有$m$个特征的输入向量（即长为$m$的一维向量），将这$m$个特征分别乘一个权重`weight`，求和后再加偏置`bias`，可以求得一个输出节点，即：
$$
y_0=x_0*w_{00}+x_1*w_{01}+...+x_m*w_{0m}+b_0
$$
全连接层希望得到$n$个输出节点，每个输出节点都对应相同数量的参数，因此全连接层共需要$(m+1)*n$个参数。

显然：

- 全连接层主要用于整合一维向量中的高阶特征
- 每个输出节点都整合了所有输入节点的信息
- 不同输出节点可以学习到输入的不同信息侧重

##### PyTorch中nn.Linear的实现

```python
import torch
import torch.nn as nn

class LinearLayer(nn.Module):
    # 全连接层仅需要输入特征的数量和输出特征的数量
    def __init__(self, in_features, out_features, bias=True):
        super(LinearLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        # 使用默认的初始化方法进行参数初始化
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, input):
        # 执行全连接层的前向传播
        return nn.functional.linear(input, self.weight, self.bias)
```

全连接模型示例如下：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 加载数据集
dataset = datasets.ImageFolder(root='path_to_dataset')

# 定义批处理大小
batch_size = 32

# 创建 DataLoader 加载数据
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, output_size)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = self.sigmoid(x)
        return x

# 创建模型实例
model = SimpleNN(input_size, output_size)

# 定义损失函数和优化器
criterion = nn.BCELoss()  # 二分类交叉熵损失
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 10

for epoch in range(num_epochs):
    for inputs, labels in dataloader:
        # 将梯度归零
        optimizer.zero_grad()

        # 前向传播
        outputs = model(inputs)
        
        # 计算损失
        loss = criterion(outputs, labels)
        
        # 反向传播
        loss.backward()
        
        # 更新权重
        optimizer.step()

    # 打印每个epoch的损失
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')

# 测试模型
with torch.no_grad():
    test_samples = torch.randn(10, input_size)
    predictions = model(test_samples)
    print("Predictions:", predictions)
```

### 卷积层(CNN Layer)
